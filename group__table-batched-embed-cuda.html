<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.7"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: CUDA Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.7 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">CUDA Operators</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga4887151424a90cfd0abef174a4e91f3f"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; at::Tensor, at::Tensor, c10::optional&lt; at::Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga4887151424a90cfd0abef174a4e91f3f">get_unique_indices_cuda</a> (at::Tensor linear_indices, int64_t max_indices, bool compute_count)</td></tr>
<tr class="separator:ga4887151424a90cfd0abef174a4e91f3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga76807cfe283a9e8f258818f3f439e6cd"><td class="memItemLeft" align="right" valign="top">std::pair&lt; at::Tensor, at::Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga76807cfe283a9e8f258818f3f439e6cd">lru_cache_find_uncached_cuda</a> (at::Tensor unique_indices, at::Tensor unique_indices_length, int64_t max_indices, at::Tensor lxu_cache_state, int64_t time_stamp, at::Tensor lru_state, bool gather_cache_stats, at::Tensor uvm_cache_stats, bool lock_cache_line, at::Tensor lxu_cache_locking_counter)</td></tr>
<tr class="separator:ga76807cfe283a9e8f258818f3f439e6cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga920da453c443675fc7fbc9d68e272a61"><td class="memItemLeft" align="right" valign="top">int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga920da453c443675fc7fbc9d68e272a61">host_lxu_cache_slot</a> (int64_t h_in, int64_t C)</td></tr>
<tr class="separator:ga920da453c443675fc7fbc9d68e272a61"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga23e7545e51b296d9b72c86f37c360dc6"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga23e7545e51b296d9b72c86f37c360dc6">linearize_cache_indices_cuda</a> (at::Tensor cache_hash_size_cumsum, at::Tensor indices, at::Tensor offsets)</td></tr>
<tr class="separator:ga23e7545e51b296d9b72c86f37c360dc6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6eed85d3e9b5dbef8a753bb81c2d6e05"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga6eed85d3e9b5dbef8a753bb81c2d6e05">linearize_cache_indices_from_row_idx_cuda</a> (at::Tensor cache_hash_size_cumsum, at::Tensor update_table_indices, at::Tensor update_row_indices)</td></tr>
<tr class="separator:ga6eed85d3e9b5dbef8a753bb81c2d6e05"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga00d12767ad238d73598bf7dc4d1afa06"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga00d12767ad238d73598bf7dc4d1afa06">lru_cache_populate_cuda</a> (at::Tensor weights, at::Tensor hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, int64_t time_stamp, at::Tensor lru_state, bool stochastic_rounding, bool gather_cache_stats, c10::optional&lt; at::Tensor &gt; uvm_cache_stats, bool lock_cache_line, c10::optional&lt; at::Tensor &gt; lxu_cache_locking_counter)</td></tr>
<tr class="separator:ga00d12767ad238d73598bf7dc4d1afa06"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5958e4cecc978d415714a3dd691fbc11"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga5958e4cecc978d415714a3dd691fbc11">lru_cache_populate_byte_cuda</a> (at::Tensor weights, at::Tensor hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor weights_tys, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, int64_t time_stamp, at::Tensor lru_state, int64_t row_alignment, bool gather_cache_stats, c10::optional&lt; at::Tensor &gt; uvm_cache_stats)</td></tr>
<tr class="separator:ga5958e4cecc978d415714a3dd691fbc11"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae019b6879bd9f89a146e0700d5a4bd8b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#gae019b6879bd9f89a146e0700d5a4bd8b">direct_mapped_lru_cache_populate_byte_cuda</a> (at::Tensor weights, at::Tensor hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor weights_tys, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, int64_t time_stamp, at::Tensor lru_state, at::Tensor lxu_cache_miss_timestamp, int64_t row_alignment, bool gather_cache_stats, c10::optional&lt; at::Tensor &gt; uvm_cache_stats)</td></tr>
<tr class="separator:gae019b6879bd9f89a146e0700d5a4bd8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga854b8951ef7e78da812be97041d7d2dc"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga854b8951ef7e78da812be97041d7d2dc">lfu_cache_populate_cuda</a> (at::Tensor weights, at::Tensor cache_hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, at::Tensor lfu_state, bool stochastic_rounding)</td></tr>
<tr class="separator:ga854b8951ef7e78da812be97041d7d2dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b76a0cf452f00e77696d896d7a402f3"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga2b76a0cf452f00e77696d896d7a402f3">lfu_cache_populate_byte_cuda</a> (at::Tensor weights, at::Tensor cache_hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor weights_tys, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, at::Tensor lfu_state, int64_t row_alignment)</td></tr>
<tr class="separator:ga2b76a0cf452f00e77696d896d7a402f3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga9ddab183e3247020b3108bfdc7d22cf9"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga9ddab183e3247020b3108bfdc7d22cf9">lxu_cache_lookup_cuda</a> (at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, int64_t invalid_index, bool gather_cache_stats, c10::optional&lt; at::Tensor &gt; uvm_cache_stats)</td></tr>
<tr class="separator:ga9ddab183e3247020b3108bfdc7d22cf9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab305ebdd3822794c5ac462bf5df4bb49"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#gab305ebdd3822794c5ac462bf5df4bb49">direct_mapped_lxu_cache_lookup_cuda</a> (at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, int64_t invalid_index, bool gather_cache_stats, c10::optional&lt; at::Tensor &gt; uvm_cache_stats)</td></tr>
<tr class="separator:gab305ebdd3822794c5ac462bf5df4bb49"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga2b055aeb5bf2d99bfb4351271764cab1">lxu_cache_flush_cuda</a> (at::Tensor uvm_weights, at::Tensor cache_hash_size_cumsum, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, int64_t total_D, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, bool stochastic_rounding)</td></tr>
<tr class="separator:ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaeaf8f13290f0fe389fefa3fc2a944311"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#gaeaf8f13290f0fe389fefa3fc2a944311">lxu_cache_locking_counter_decrement_cuda</a> (at::Tensor lxu_cache_locking_counter, at::Tensor lxu_cache_locations)</td></tr>
<tr class="separator:gaeaf8f13290f0fe389fefa3fc2a944311"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga886ce990864a83356e9eed06b9831f47"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga886ce990864a83356e9eed06b9831f47">lxu_cache_locations_update_cuda</a> (at::Tensor lxu_cache_locations, at::Tensor lxu_cache_locations_new)</td></tr>
<tr class="separator:ga886ce990864a83356e9eed06b9831f47"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>The following are CUDA Operators </p>
<h2 class="groupheader">Function Documentation</h2>
<a id="gae019b6879bd9f89a146e0700d5a4bd8b" name="gae019b6879bd9f89a146e0700d5a4bd8b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gae019b6879bd9f89a146e0700d5a4bd8b">&#9670;&#160;</a></span>direct_mapped_lru_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void direct_mapped_lru_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_tys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_miss_timestamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>row_alignment</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gather_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; at::Tensor &gt;&#160;</td>
          <td class="paramname"><em>uvm_cache_stats</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Direct-mapped (assoc=1) variant of lru_cache_populate_byte_cuda </p>

</div>
</div>
<a id="gab305ebdd3822794c5ac462bf5df4bb49" name="gab305ebdd3822794c5ac462bf5df4bb49"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gab305ebdd3822794c5ac462bf5df4bb49">&#9670;&#160;</a></span>direct_mapped_lxu_cache_lookup_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor direct_mapped_lxu_cache_lookup_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>invalid_index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gather_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; at::Tensor &gt;&#160;</td>
          <td class="paramname"><em>uvm_cache_stats</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup the LRU/LFU cache: find the cache weights location for all indices. Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with a sentinel value for missing. </p>

</div>
</div>
<a id="ga4887151424a90cfd0abef174a4e91f3f" name="ga4887151424a90cfd0abef174a4e91f3f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga4887151424a90cfd0abef174a4e91f3f">&#9670;&#160;</a></span>get_unique_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; at::Tensor, at::Tensor, c10::optional&lt; at::Tensor &gt; &gt; get_unique_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>max_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>compute_count</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Deduplicate indices. </p>

</div>
</div>
<a id="ga920da453c443675fc7fbc9d68e272a61" name="ga920da453c443675fc7fbc9d68e272a61"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga920da453c443675fc7fbc9d68e272a61">&#9670;&#160;</a></span>host_lxu_cache_slot()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int64_t host_lxu_cache_slot </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>h_in</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>C</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map index to cache_set. h_in: linear_indices; C: #cache_sets. </p>

</div>
</div>
<a id="ga2b76a0cf452f00e77696d896d7a402f3" name="ga2b76a0cf452f00e77696d896d7a402f3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b76a0cf452f00e77696d896d7a402f3">&#9670;&#160;</a></span>lfu_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lfu_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_tys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lfu_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>row_alignment</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache. weights and lxu_cache_weights have "uint8_t" byte elements </p>

</div>
</div>
<a id="ga854b8951ef7e78da812be97041d7d2dc" name="ga854b8951ef7e78da812be97041d7d2dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga854b8951ef7e78da812be97041d7d2dc">&#9670;&#160;</a></span>lfu_cache_populate_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lfu_cache_populate_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lfu_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stochastic_rounding</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache. </p>

</div>
</div>
<a id="ga23e7545e51b296d9b72c86f37c360dc6" name="ga23e7545e51b296d9b72c86f37c360dc6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga23e7545e51b296d9b72c86f37c360dc6">&#9670;&#160;</a></span>linearize_cache_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor linearize_cache_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>offsets</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Linearize the indices of all tables to make it be unique </p>

</div>
</div>
<a id="ga6eed85d3e9b5dbef8a753bb81c2d6e05" name="ga6eed85d3e9b5dbef8a753bb81c2d6e05"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6eed85d3e9b5dbef8a753bb81c2d6e05">&#9670;&#160;</a></span>linearize_cache_indices_from_row_idx_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor linearize_cache_indices_from_row_idx_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>update_table_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>update_row_indices</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Linearize the indices of all tables to make it be unique. Note the update_table_indices and update_row_indices are from the row indices format for inplace update. </p>

</div>
</div>
<a id="ga76807cfe283a9e8f258818f3f439e6cd" name="ga76807cfe283a9e8f258818f3f439e6cd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga76807cfe283a9e8f258818f3f439e6cd">&#9670;&#160;</a></span>lru_cache_find_uncached_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::pair&lt; at::Tensor, at::Tensor &gt; lru_cache_find_uncached_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>unique_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>unique_indices_length</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>max_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gather_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>uvm_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>lock_cache_line</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_locking_counter</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup LRU cache to find uncached indices, and then sort them based on the set. </p>

</div>
</div>
<a id="ga5958e4cecc978d415714a3dd691fbc11" name="ga5958e4cecc978d415714a3dd691fbc11"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga5958e4cecc978d415714a3dd691fbc11">&#9670;&#160;</a></span>lru_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lru_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_tys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>row_alignment</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gather_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; at::Tensor &gt;&#160;</td>
          <td class="paramname"><em>uvm_cache_stats</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. weights and lxu_cache_weights have "uint8_t" byte elements </p>

</div>
</div>
<a id="ga00d12767ad238d73598bf7dc4d1afa06" name="ga00d12767ad238d73598bf7dc4d1afa06"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga00d12767ad238d73598bf7dc4d1afa06">&#9670;&#160;</a></span>lru_cache_populate_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lru_cache_populate_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stochastic_rounding</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gather_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; at::Tensor &gt;&#160;</td>
          <td class="paramname"><em>uvm_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>lock_cache_line</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; at::Tensor &gt;&#160;</td>
          <td class="paramname"><em>lxu_cache_locking_counter</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. </p>

</div>
</div>
<a id="ga2b055aeb5bf2d99bfb4351271764cab1" name="ga2b055aeb5bf2d99bfb4351271764cab1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b055aeb5bf2d99bfb4351271764cab1">&#9670;&#160;</a></span>lxu_cache_flush_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_flush_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>uvm_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_D</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stochastic_rounding</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Flush the cache: store the weights from the cache to the backing storage. </p>

</div>
</div>
<a id="ga886ce990864a83356e9eed06b9831f47" name="ga886ce990864a83356e9eed06b9831f47"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga886ce990864a83356e9eed06b9831f47">&#9670;&#160;</a></span>lxu_cache_locations_update_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_locations_update_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_locations</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_locations_new</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Inplace update lxu_cache_locations to the new one should only update if lxu_cache_locations[i] == -1 and lxu_cache_locations_new[i] &gt;= 0 </p>

</div>
</div>
<a id="gaeaf8f13290f0fe389fefa3fc2a944311" name="gaeaf8f13290f0fe389fefa3fc2a944311"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaeaf8f13290f0fe389fefa3fc2a944311">&#9670;&#160;</a></span>lxu_cache_locking_counter_decrement_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_locking_counter_decrement_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_locking_counter</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_locations</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Decrement the LRU/LFU cache counter based on lxu_cache_locations. </p>

</div>
</div>
<a id="ga9ddab183e3247020b3108bfdc7d22cf9" name="ga9ddab183e3247020b3108bfdc7d22cf9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga9ddab183e3247020b3108bfdc7d22cf9">&#9670;&#160;</a></span>lxu_cache_lookup_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor lxu_cache_lookup_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>invalid_index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gather_cache_stats</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; at::Tensor &gt;&#160;</td>
          <td class="paramname"><em>uvm_cache_stats</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup the LRU/LFU cache: find the cache weights location for all indices. Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with a sentinel value for missing. </p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.7
</small></address>
</body>
</html>
