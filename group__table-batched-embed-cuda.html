<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: CUDA Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">CUDA Operators</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga4887151424a90cfd0abef174a4e91f3f"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; at::Tensor, at::Tensor, c10::optional&lt; at::Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga4887151424a90cfd0abef174a4e91f3f">get_unique_indices_cuda</a> (at::Tensor linear_indices, int64_t max_indices, bool compute_count)</td></tr>
<tr class="separator:ga4887151424a90cfd0abef174a4e91f3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga53ca6da68dfef1a3cec39809ead0eeea"><td class="memItemLeft" align="right" valign="top">std::pair&lt; at::Tensor, at::Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga53ca6da68dfef1a3cec39809ead0eeea">lru_cache_find_uncached_cuda</a> (at::Tensor unique_indices, at::Tensor unique_indices_length, int64_t max_indices, at::Tensor lxu_cache_state, int64_t time_stamp, at::Tensor lru_state)</td></tr>
<tr class="separator:ga53ca6da68dfef1a3cec39809ead0eeea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga920da453c443675fc7fbc9d68e272a61"><td class="memItemLeft" align="right" valign="top">int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga920da453c443675fc7fbc9d68e272a61">host_lxu_cache_slot</a> (int64_t h_in, int64_t C)</td></tr>
<tr class="separator:ga920da453c443675fc7fbc9d68e272a61"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga23e7545e51b296d9b72c86f37c360dc6"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga23e7545e51b296d9b72c86f37c360dc6">linearize_cache_indices_cuda</a> (at::Tensor cache_hash_size_cumsum, at::Tensor indices, at::Tensor offsets)</td></tr>
<tr class="separator:ga23e7545e51b296d9b72c86f37c360dc6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad4d6509a6d55a02ccbe868a8c5407ea5"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#gad4d6509a6d55a02ccbe868a8c5407ea5">lru_cache_populate_cuda</a> (at::Tensor weights, at::Tensor hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, int64_t time_stamp, at::Tensor lru_state, bool stochastic_rounding)</td></tr>
<tr class="separator:gad4d6509a6d55a02ccbe868a8c5407ea5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga53373b6a4ae3f373172a671d46222ed4"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga53373b6a4ae3f373172a671d46222ed4">lru_cache_populate_byte_cuda</a> (at::Tensor weights, at::Tensor hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor weights_tys, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, int64_t time_stamp, at::Tensor lru_state, int64_t row_alignment)</td></tr>
<tr class="separator:ga53373b6a4ae3f373172a671d46222ed4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga854b8951ef7e78da812be97041d7d2dc"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga854b8951ef7e78da812be97041d7d2dc">lfu_cache_populate_cuda</a> (at::Tensor weights, at::Tensor cache_hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, at::Tensor lfu_state, bool stochastic_rounding)</td></tr>
<tr class="separator:ga854b8951ef7e78da812be97041d7d2dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b76a0cf452f00e77696d896d7a402f3"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga2b76a0cf452f00e77696d896d7a402f3">lfu_cache_populate_byte_cuda</a> (at::Tensor weights, at::Tensor cache_hash_size_cumsum, int64_t total_cache_hash_size, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor weights_tys, at::Tensor D_offsets, at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, at::Tensor lfu_state, int64_t row_alignment)</td></tr>
<tr class="separator:ga2b76a0cf452f00e77696d896d7a402f3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2a5254ad909c613007ffc24ac178aba4"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga2a5254ad909c613007ffc24ac178aba4">lxu_cache_lookup_cuda</a> (at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, int64_t invalid_index)</td></tr>
<tr class="separator:ga2a5254ad909c613007ffc24ac178aba4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__table-batched-embed-cuda.html#ga2b055aeb5bf2d99bfb4351271764cab1">lxu_cache_flush_cuda</a> (at::Tensor uvm_weights, at::Tensor cache_hash_size_cumsum, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, int64_t total_D, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, bool stochastic_rounding)</td></tr>
<tr class="separator:ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p >The following are CUDA Operators </p>
<h2 class="groupheader">Function Documentation</h2>
<a id="ga4887151424a90cfd0abef174a4e91f3f" name="ga4887151424a90cfd0abef174a4e91f3f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga4887151424a90cfd0abef174a4e91f3f">&#9670;&#160;</a></span>get_unique_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; at::Tensor, at::Tensor, c10::optional&lt; at::Tensor &gt; &gt; get_unique_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>max_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>compute_count</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Deduplicate indices. </p>

</div>
</div>
<a id="ga920da453c443675fc7fbc9d68e272a61" name="ga920da453c443675fc7fbc9d68e272a61"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga920da453c443675fc7fbc9d68e272a61">&#9670;&#160;</a></span>host_lxu_cache_slot()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int64_t host_lxu_cache_slot </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>h_in</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>C</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Map index to cache_set. h_in: linear_indices; C: #cache_sets. </p>

</div>
</div>
<a id="ga2b76a0cf452f00e77696d896d7a402f3" name="ga2b76a0cf452f00e77696d896d7a402f3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b76a0cf452f00e77696d896d7a402f3">&#9670;&#160;</a></span>lfu_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lfu_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_tys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lfu_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>row_alignment</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache. weights and lxu_cache_weights have "uint8_t" byte elements </p>

</div>
</div>
<a id="ga854b8951ef7e78da812be97041d7d2dc" name="ga854b8951ef7e78da812be97041d7d2dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga854b8951ef7e78da812be97041d7d2dc">&#9670;&#160;</a></span>lfu_cache_populate_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lfu_cache_populate_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lfu_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stochastic_rounding</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache. </p>

</div>
</div>
<a id="ga23e7545e51b296d9b72c86f37c360dc6" name="ga23e7545e51b296d9b72c86f37c360dc6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga23e7545e51b296d9b72c86f37c360dc6">&#9670;&#160;</a></span>linearize_cache_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor linearize_cache_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>offsets</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Linearize the indices of all tables to make it be unique </p>

</div>
</div>
<a id="ga53ca6da68dfef1a3cec39809ead0eeea" name="ga53ca6da68dfef1a3cec39809ead0eeea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga53ca6da68dfef1a3cec39809ead0eeea">&#9670;&#160;</a></span>lru_cache_find_uncached_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::pair&lt; at::Tensor, at::Tensor &gt; lru_cache_find_uncached_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>unique_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>unique_indices_length</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>max_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Lookup LRU cache to find uncached indices, and then sort them based on the set. </p>

</div>
</div>
<a id="ga53373b6a4ae3f373172a671d46222ed4" name="ga53373b6a4ae3f373172a671d46222ed4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga53373b6a4ae3f373172a671d46222ed4">&#9670;&#160;</a></span>lru_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lru_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_tys</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>row_alignment</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. weights and lxu_cache_weights have "uint8_t" byte elements </p>

</div>
</div>
<a id="gad4d6509a6d55a02ccbe868a8c5407ea5" name="gad4d6509a6d55a02ccbe868a8c5407ea5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad4d6509a6d55a02ccbe868a8c5407ea5">&#9670;&#160;</a></span>lru_cache_populate_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lru_cache_populate_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_cache_hash_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>time_stamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lru_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stochastic_rounding</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. </p>

</div>
</div>
<a id="ga2b055aeb5bf2d99bfb4351271764cab1" name="ga2b055aeb5bf2d99bfb4351271764cab1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b055aeb5bf2d99bfb4351271764cab1">&#9670;&#160;</a></span>lxu_cache_flush_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_flush_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>uvm_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_hash_size_cumsum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>cache_index_table_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>weights_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>D_offsets</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>total_D</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stochastic_rounding</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Flush the cache: store the weights from the cache to the backing storage. </p>

</div>
</div>
<a id="ga2a5254ad909c613007ffc24ac178aba4" name="ga2a5254ad909c613007ffc24ac178aba4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2a5254ad909c613007ffc24ac178aba4">&#9670;&#160;</a></span>lxu_cache_lookup_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor lxu_cache_lookup_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>linear_cache_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor&#160;</td>
          <td class="paramname"><em>lxu_cache_state</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>invalid_index</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Lookup the LRU/LFU cache: find the cache weights location for all indices. Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with a sentinel value for missing. </p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5
</small></address>
</body>
</html>
