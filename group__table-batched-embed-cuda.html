<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: CUDA Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">CUDA Operators</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga4887151424a90cfd0abef174a4e91f3f" id="r_ga4887151424a90cfd0abef174a4e91f3f"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga4887151424a90cfd0abef174a4e91f3f">get_unique_indices_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">linear_indices</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">max_indices</a>, <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">compute_count</a>)</td></tr>
<tr class="separator:ga4887151424a90cfd0abef174a4e91f3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga76807cfe283a9e8f258818f3f439e6cd" id="r_ga76807cfe283a9e8f258818f3f439e6cd"><td class="memItemLeft" align="right" valign="top">std::pair&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga76807cfe283a9e8f258818f3f439e6cd">lru_cache_find_uncached_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">unique_indices</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">unique_indices_length</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">max_indices</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lxu_cache_state</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">time_stamp</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lru_state</a>, <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">gather_cache_stats</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">uvm_cache_stats</a>, <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lock_cache_line</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lxu_cache_locking_counter</a>)</td></tr>
<tr class="separator:ga76807cfe283a9e8f258818f3f439e6cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga920da453c443675fc7fbc9d68e272a61" id="r_ga920da453c443675fc7fbc9d68e272a61"><td class="memItemLeft" align="right" valign="top"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga920da453c443675fc7fbc9d68e272a61">host_lxu_cache_slot</a> (<a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">h_in</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">C</a>)</td></tr>
<tr class="separator:ga920da453c443675fc7fbc9d68e272a61"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga23e7545e51b296d9b72c86f37c360dc6" id="r_ga23e7545e51b296d9b72c86f37c360dc6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga23e7545e51b296d9b72c86f37c360dc6">linearize_cache_indices_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_hash_size_cumsum</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>)</td></tr>
<tr class="separator:ga23e7545e51b296d9b72c86f37c360dc6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6eed85d3e9b5dbef8a753bb81c2d6e05" id="r_ga6eed85d3e9b5dbef8a753bb81c2d6e05"><td class="memItemLeft" align="right" valign="top"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga6eed85d3e9b5dbef8a753bb81c2d6e05">linearize_cache_indices_from_row_idx_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_hash_size_cumsum</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">update_table_indices</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">update_row_indices</a>)</td></tr>
<tr class="separator:ga6eed85d3e9b5dbef8a753bb81c2d6e05"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga00d12767ad238d73598bf7dc4d1afa06" id="r_ga00d12767ad238d73598bf7dc4d1afa06"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga00d12767ad238d73598bf7dc4d1afa06">lru_cache_populate_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> hash_size_cumsum, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> total_cache_hash_size, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> cache_index_table_map, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linear_cache_indices, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_state, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> time_stamp, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lru_state, bool <a class="el" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a5cc1b5faf7430930527acfac8e6b8068">stochastic_rounding</a>, bool gather_cache_stats, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; uvm_cache_stats, bool lock_cache_line, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; lxu_cache_locking_counter)</td></tr>
<tr class="separator:ga00d12767ad238d73598bf7dc4d1afa06"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5958e4cecc978d415714a3dd691fbc11" id="r_ga5958e4cecc978d415714a3dd691fbc11"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga5958e4cecc978d415714a3dd691fbc11">lru_cache_populate_byte_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> hash_size_cumsum, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> total_cache_hash_size, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> cache_index_table_map, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights_tys, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linear_cache_indices, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_state, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> time_stamp, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lru_state, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> row_alignment, bool gather_cache_stats, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; uvm_cache_stats)</td></tr>
<tr class="separator:ga5958e4cecc978d415714a3dd691fbc11"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae019b6879bd9f89a146e0700d5a4bd8b" id="r_gae019b6879bd9f89a146e0700d5a4bd8b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gae019b6879bd9f89a146e0700d5a4bd8b">direct_mapped_lru_cache_populate_byte_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> hash_size_cumsum, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> total_cache_hash_size, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> cache_index_table_map, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights_tys, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linear_cache_indices, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_state, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> time_stamp, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lru_state, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_miss_timestamp, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> row_alignment, bool gather_cache_stats, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; uvm_cache_stats)</td></tr>
<tr class="separator:gae019b6879bd9f89a146e0700d5a4bd8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga854b8951ef7e78da812be97041d7d2dc" id="r_ga854b8951ef7e78da812be97041d7d2dc"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga854b8951ef7e78da812be97041d7d2dc">lfu_cache_populate_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_hash_size_cumsum</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">total_cache_hash_size</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_index_table_map</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">linear_cache_indices</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lxu_cache_state</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lfu_state</a>, <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a> <a class="el" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a5cc1b5faf7430930527acfac8e6b8068">stochastic_rounding</a>)</td></tr>
<tr class="separator:ga854b8951ef7e78da812be97041d7d2dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b76a0cf452f00e77696d896d7a402f3" id="r_ga2b76a0cf452f00e77696d896d7a402f3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga2b76a0cf452f00e77696d896d7a402f3">lfu_cache_populate_byte_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> weights, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_hash_size_cumsum</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">total_cache_hash_size</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_index_table_map</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">weights_tys</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">linear_cache_indices</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lxu_cache_state</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lfu_state</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">row_alignment</a>)</td></tr>
<tr class="separator:ga2b76a0cf452f00e77696d896d7a402f3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga124b70b0fede88f508e59111ce6d765f" id="r_ga124b70b0fede88f508e59111ce6d765f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga124b70b0fede88f508e59111ce6d765f">lxu_cache_lookup_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linear_cache_indices, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_state, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> invalid_index, bool gather_cache_stats, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; uvm_cache_stats, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; num_uniq_cache_indices, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; lxu_cache_locations_output)</td></tr>
<tr class="separator:ga124b70b0fede88f508e59111ce6d765f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab305ebdd3822794c5ac462bf5df4bb49" id="r_gab305ebdd3822794c5ac462bf5df4bb49"><td class="memItemLeft" align="right" valign="top"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gab305ebdd3822794c5ac462bf5df4bb49">direct_mapped_lxu_cache_lookup_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linear_cache_indices, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_state, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> invalid_index, bool gather_cache_stats, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; uvm_cache_stats)</td></tr>
<tr class="separator:gab305ebdd3822794c5ac462bf5df4bb49"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b055aeb5bf2d99bfb4351271764cab1" id="r_ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga2b055aeb5bf2d99bfb4351271764cab1">lxu_cache_flush_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_hash_size_cumsum</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">cache_index_table_map</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, <a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">total_D</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lxu_cache_state</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a> <a class="el" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a5cc1b5faf7430930527acfac8e6b8068">stochastic_rounding</a>)</td></tr>
<tr class="separator:ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaeaf8f13290f0fe389fefa3fc2a944311" id="r_gaeaf8f13290f0fe389fefa3fc2a944311"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gaeaf8f13290f0fe389fefa3fc2a944311">lxu_cache_locking_counter_decrement_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">lxu_cache_locking_counter</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>)</td></tr>
<tr class="separator:gaeaf8f13290f0fe389fefa3fc2a944311"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga65cba33a439fb1ed50fe2e80dc22b603" id="r_ga65cba33a439fb1ed50fe2e80dc22b603"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga65cba33a439fb1ed50fe2e80dc22b603">lxu_cache_locations_update_cuda</a> (<a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> <a class="el" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_locations_new, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; num_uniq_cache_indices)</td></tr>
<tr class="separator:ga65cba33a439fb1ed50fe2e80dc22b603"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>The following are CUDA Operators </p>
<h2 class="groupheader">Function Documentation</h2>
<a id="gae019b6879bd9f89a146e0700d5a4bd8b" name="gae019b6879bd9f89a146e0700d5a4bd8b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gae019b6879bd9f89a146e0700d5a4bd8b">&#9670;&#160;</a></span>direct_mapped_lru_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void direct_mapped_lru_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_cache_hash_size</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_tys</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>time_stamp</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lru_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_miss_timestamp</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>row_alignment</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Direct-mapped (assoc=1) variant of lru_cache_populate_byte_cuda </p>

</div>
</div>
<a id="gab305ebdd3822794c5ac462bf5df4bb49" name="gab305ebdd3822794c5ac462bf5df4bb49"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gab305ebdd3822794c5ac462bf5df4bb49">&#9670;&#160;</a></span>direct_mapped_lxu_cache_lookup_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> direct_mapped_lxu_cache_lookup_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>invalid_index</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup the LRU/LFU cache: find the cache weights location for all indices. Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with a sentinel value for missing. </p>

</div>
</div>
<a id="ga4887151424a90cfd0abef174a4e91f3f" name="ga4887151424a90cfd0abef174a4e91f3f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga4887151424a90cfd0abef174a4e91f3f">&#9670;&#160;</a></span>get_unique_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>, c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; &gt; get_unique_indices_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>max_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a></td>          <td class="paramname"><span class="paramname"><em>compute_count</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Deduplicate indices. </p>

</div>
</div>
<a id="ga920da453c443675fc7fbc9d68e272a61" name="ga920da453c443675fc7fbc9d68e272a61"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga920da453c443675fc7fbc9d68e272a61">&#9670;&#160;</a></span>host_lxu_cache_slot()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a> host_lxu_cache_slot </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>h_in</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>C</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map index to cache_set. h_in: linear_indices; C: #cache_sets. </p>

</div>
</div>
<a id="ga2b76a0cf452f00e77696d896d7a402f3" name="ga2b76a0cf452f00e77696d896d7a402f3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b76a0cf452f00e77696d896d7a402f3">&#9670;&#160;</a></span>lfu_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a> lfu_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_cache_hash_size</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_tys</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lfu_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>row_alignment</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache. weights and lxu_cache_weights have "uint8_t" byte elements </p>

</div>
</div>
<a id="ga854b8951ef7e78da812be97041d7d2dc" name="ga854b8951ef7e78da812be97041d7d2dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga854b8951ef7e78da812be97041d7d2dc">&#9670;&#160;</a></span>lfu_cache_populate_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a> lfu_cache_populate_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_cache_hash_size</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lfu_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a></td>          <td class="paramname"><span class="paramname"><em>stochastic_rounding</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache. </p>

</div>
</div>
<a id="ga23e7545e51b296d9b72c86f37c360dc6" name="ga23e7545e51b296d9b72c86f37c360dc6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga23e7545e51b296d9b72c86f37c360dc6">&#9670;&#160;</a></span>linearize_cache_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linearize_cache_indices_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>offsets</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Linearize the indices of all tables to make it be unique </p>

</div>
</div>
<a id="ga6eed85d3e9b5dbef8a753bb81c2d6e05" name="ga6eed85d3e9b5dbef8a753bb81c2d6e05"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6eed85d3e9b5dbef8a753bb81c2d6e05">&#9670;&#160;</a></span>linearize_cache_indices_from_row_idx_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> linearize_cache_indices_from_row_idx_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>update_table_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>update_row_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Linearize the indices of all tables to make it be unique. Note the update_table_indices and update_row_indices are from the row indices format for inplace update. </p>

</div>
</div>
<a id="ga76807cfe283a9e8f258818f3f439e6cd" name="ga76807cfe283a9e8f258818f3f439e6cd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga76807cfe283a9e8f258818f3f439e6cd">&#9670;&#160;</a></span>lru_cache_find_uncached_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::pair&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a>, <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt; lru_cache_find_uncached_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>unique_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>unique_indices_length</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>max_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>time_stamp</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lru_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a></td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a></td>          <td class="paramname"><span class="paramname"><em>lock_cache_line</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locking_counter</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup LRU cache to find uncached indices, and then sort them based on the set. </p>

</div>
</div>
<a id="ga5958e4cecc978d415714a3dd691fbc11" name="ga5958e4cecc978d415714a3dd691fbc11"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga5958e4cecc978d415714a3dd691fbc11">&#9670;&#160;</a></span>lru_cache_populate_byte_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lru_cache_populate_byte_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_cache_hash_size</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_tys</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>time_stamp</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lru_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>row_alignment</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. weights and lxu_cache_weights have "uint8_t" byte elements </p>

</div>
</div>
<a id="ga00d12767ad238d73598bf7dc4d1afa06" name="ga00d12767ad238d73598bf7dc4d1afa06"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga00d12767ad238d73598bf7dc4d1afa06">&#9670;&#160;</a></span>lru_cache_populate_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lru_cache_populate_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_cache_hash_size</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>time_stamp</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lru_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>stochastic_rounding</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>lock_cache_line</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locking_counter</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. </p>

</div>
</div>
<a id="ga2b055aeb5bf2d99bfb4351271764cab1" name="ga2b055aeb5bf2d99bfb4351271764cab1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b055aeb5bf2d99bfb4351271764cab1">&#9670;&#160;</a></span>lxu_cache_flush_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a> lxu_cache_flush_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>uvm_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_D</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">bool</a></td>          <td class="paramname"><span class="paramname"><em>stochastic_rounding</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Flush the cache: store the weights from the cache to the backing storage. </p>

</div>
</div>
<a id="ga65cba33a439fb1ed50fe2e80dc22b603" name="ga65cba33a439fb1ed50fe2e80dc22b603"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga65cba33a439fb1ed50fe2e80dc22b603">&#9670;&#160;</a></span>lxu_cache_locations_update_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_locations_update_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations_new</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>num_uniq_cache_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Inplace update lxu_cache_locations to the new one should only update if lxu_cache_locations[i] == -1 and lxu_cache_locations_new[i] &gt;= 0 </p>

</div>
</div>
<a id="gaeaf8f13290f0fe389fefa3fc2a944311" name="gaeaf8f13290f0fe389fefa3fc2a944311"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaeaf8f13290f0fe389fefa3fc2a944311">&#9670;&#160;</a></span>lxu_cache_locking_counter_decrement_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a> lxu_cache_locking_counter_decrement_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locking_counter</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Decrement the LRU/LFU cache counter based on lxu_cache_locations. </p>

</div>
</div>
<a id="ga124b70b0fede88f508e59111ce6d765f" name="ga124b70b0fede88f508e59111ce6d765f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga124b70b0fede88f508e59111ce6d765f">&#9670;&#160;</a></span>lxu_cache_lookup_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> lxu_cache_lookup_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>invalid_index</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>num_uniq_cache_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">c10::optional&lt; <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a> &gt;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations_output</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup the LRU/LFU cache: find the cache weights location for all indices. Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with a sentinel value for missing. </p>

</div>
</div>
<a id="ga59334fdad832f8d67576e6c83a9b9d79" name="ga59334fdad832f8d67576e6c83a9b9d79"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga59334fdad832f8d67576e6c83a9b9d79">&#9670;&#160;</a></span>reset_weight_momentum_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classfbgemm__gpu_1_1_tensor_accessor.html">void</a> reset_weight_momentum_cuda </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>dev_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>uvm_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_placements</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>momentum1_dev</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>momentum1_uvm</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>momentum1_placements</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>momentum1_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>D_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>pruned_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>pruned_indices_offsets</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>logical_table_ids</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>buffer_ids</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">at::Tensor</a></td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></td>          <td class="paramname"><span class="paramname"><em>total_cache_hash_size</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0
</small></address>
</body>
</html>
