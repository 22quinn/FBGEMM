<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: /__w/FBGEMM/FBGEMM/fbgemm_gpu/codegen/embedding_forward_quantized_split_nbit_host_template.cu File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_0255d041b3ce7964bcd7b11954959c22.html">codegen</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a>  </div>
  <div class="headertitle"><div class="title">embedding_forward_quantized_split_nbit_host_template.cu File Reference</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><code>#include &quot;<a class="el" href="embedding__forward__template__helpers_8cuh.html">codegen/embedding_forward_template_helpers.cuh</a>&quot;</code><br />
<code>#include &quot;<a class="el" href="fbgemm__tensor__accessor_8h.html">fbgemm_gpu/fbgemm_tensor_accessor.h</a>&quot;</code><br />
</div><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="namespaces" name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespacenbit" id="r_namespacenbit"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacenbit.html">nbit</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Macro Definition Documentation</h2>
<a id="ae6cc33dae61d3333c3d2e6be5f9cf16e" name="ae6cc33dae61d3333c3d2e6be5f9cf16e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6cc33dae61d3333c3d2e6be5f9cf16e">&#9670;&#160;</a></span>X <span class="overload">[1/6]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define X</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">DeviceOnly, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">OutputRowsPerThread, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">InputRowsInFlight, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MinNum128BRows, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MaxNum128BRows</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">    nbit::INT2_split_embedding{{ <span class="stringliteral">&quot;_nobag&quot;</span> <span class="keywordflow">if</span> nobag <span class="keywordflow">else</span> <span class="stringliteral">&quot;&quot;</span> }}_codegen_forward_{{ wdesc }}_kernel_small_L&lt;index_t, output_t, OutputRowsPerThread, kWarpsPerBlock, InputRowsInFlight, MinNum128BRows, MaxNum128BRows, DeviceOnly&gt;&lt;&lt;&lt; \</div>
<div class="line">        nbit::div_round_up(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a> * <a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread), kWarpsPerBlock), \</div>
<div class="line">        dim3(<a class="code hl_variable" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a>, kWarpsPerBlock), \</div>
<div class="line">        0, \</div>
<div class="line">        at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;( \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a>, int32_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, weights_tys, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, int32_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">else</span> %} \</div>
<div class="line">        <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a>, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        FixedDivisor(<a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread)), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, index_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>, index_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        pooling_mode, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        row_alignment, \</div>
<div class="line">        {% <span class="keywordflow">if</span> weighted %} <a class="code hl_define" href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a>(func_name1, indice_weights, <span class="keywordtype">float</span>, 1, 32), {% endif %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a>, output_t, 2, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 2, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name1, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, int32_t, 1, 32) \</div>
<div class="line">    ); \</div>
<div class="line">    C10_CUDA_KERNEL_LAUNCH_CHECK(); \</div>
<div class="ttc" id="afbgemm__tensor__accessor_8h_html_a614f4b016e2758186bd598bc3be6e6cf"><div class="ttname"><a href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a></div><div class="ttdeci">#define MAKE_PTA_WITH_NAME(FUNC_NAME, TENSOR, T, N, INDEX_NBITS)</div><div class="ttdef"><b>Definition</b> fbgemm_tensor_accessor.h:577</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel_8cu_html_a1360e7840ee58417b26bf9445f94c59d"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a></div><div class="ttdeci">template uint8_t</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel.cu:1240</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel_8cu_html_ac4ebc0de2e60165af8333b6f4eab3e70"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a></div><div class="ttdeci">template int64_t</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel.cu:1241</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel__small_8cu_html_a110a71f81fecd3888738618492db1672"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int32_t, 1, at::RestrictPtrTraits &gt; FixedDivisor const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const int32_t const bool pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; output</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel_small.cu:128</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel__small_8cu_html_a6d8072fe7f1cbd1cf456e3ea8a440ad3"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; dev_weights</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel_small.cu:119</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel__small_8cu_html_a764f8ae801cd000c2a5cb4bb23f14299"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; weights_offsets</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel_small.cu:120</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel__small_8cu_html_a8a3ac708f5fc38ea5ebecdbe685f3c73"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int32_t, 1, at::RestrictPtrTraits &gt; D_offsets</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel_small.cu:121</div></div>
<div class="ttc" id="agen__batch__index__select__dim0__forward__kernel__small_8cu_html_acbf20500022fb5f972956bea423a05ff"><div class="ttname"><a href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int32_t, 1, at::RestrictPtrTraits &gt; FixedDivisor const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; indices</div><div class="ttdef"><b>Definition</b> gen_batch_index_select_dim0_forward_kernel_small.cu:123</div></div>
<div class="ttc" id="agen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu_html_a057f5488fcdaf454d09c4f1b25374ac9"><div class="ttname"><a href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; int64_t D</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_dense_unweighted_nobag_kernel_small.cu:101</div></div>
<div class="ttc" id="agen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu_html_aff2584a62b3409906c19c5419a4cc647"><div class="ttname"><a href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a></div><div class="ttdeci">template const pta::PackedTensorAccessor64&lt; uint8_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; int64_t FixedDivisor const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; const pta::PackedTensorAccessor32&lt; int64_t, 1, at::RestrictPtrTraits &gt; offsets</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_dense_unweighted_nobag_kernel_small.cu:104</div></div>
<div class="ttc" id="agen__embedding__forward__split__unweighted__codegen__cuda_8cu_html_a0c2527424502280dfcf6276b49b41cdc"><div class="ttname"><a href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a></div><div class="ttdeci">__launch_bounds__(kForwardMaxThreads) __global__ void split_embedding_nobag_codegen_forward_unweighted_small_kernel(const pta const emb_t *__restrict__ const const cache_t *__restrict__ const lxu_cache_weights</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_split_unweighted_codegen_cuda.cu:58</div></div>
<div class="ttc" id="agen__embedding__forward__split__unweighted__codegen__cuda_8cu_html_a17f61eb7bf7a7e4089982fbf69116da5"><div class="ttname"><a href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a></div><div class="ttdeci">__launch_bounds__(kForwardMaxThreads) __global__ void split_embedding_nobag_codegen_forward_unweighted_small_kernel(const pta const emb_t *__restrict__ const uvm_weights</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_split_unweighted_codegen_cuda.cu:57</div></div>
<div class="ttc" id="agen__embedding__forward__split__unweighted__codegen__cuda_8cu_html_a240b4e029c521f922d447346c8b757b8"><div class="ttname"><a href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a></div><div class="ttdeci">__launch_bounds__(kForwardMaxThreads) __global__ void split_embedding_nobag_codegen_forward_unweighted_small_kernel(const pta const emb_t *__restrict__ const const cache_t *__restrict__ const const int32_t *__restrict__ const const uint32_t B</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_split_unweighted_codegen_cuda.cu:60</div></div>
<div class="ttc" id="agen__embedding__forward__split__unweighted__codegen__cuda_8cu_html_a2ee4b3e799d56c4d34c87190c37a7a64"><div class="ttname"><a href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a></div><div class="ttdeci">__launch_bounds__(kForwardMaxThreads) __global__ void split_embedding_nobag_codegen_forward_unweighted_small_kernel(const pta const emb_t *__restrict__ const const cache_t *__restrict__ const const int32_t *__restrict__ const const uint32_t const uint32_t T</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_split_unweighted_codegen_cuda.cu:61</div></div>
<div class="ttc" id="agen__embedding__forward__split__unweighted__codegen__cuda_8cu_html_a60a1ec59d36df78e844d5cd7a0d34f03"><div class="ttname"><a href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a></div><div class="ttdeci">__launch_bounds__(kForwardMaxThreads) __global__ void split_embedding_nobag_codegen_forward_unweighted_small_kernel(const pta const emb_t *__restrict__ const const cache_t *__restrict__ const const int32_t *__restrict__ const const uint32_t const uint32_t const bool const uint32_t const FixedDivisor const index_t *__restrict__ const const index_t *__restrict__ const const uint32_t *__restrict__ const const int64_t *__restrict__ const const int32_t *__restrict__ const lxu_cache_locations</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_split_unweighted_codegen_cuda.cu:69</div></div>
<div class="ttc" id="agen__embedding__forward__split__unweighted__codegen__cuda_8cu_html_ad4dd9cc51f1eccdf4626318632701868"><div class="ttname"><a href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a></div><div class="ttdeci">__launch_bounds__(kForwardMaxThreads) __global__ void split_embedding_nobag_codegen_forward_unweighted_small_kernel(const pta const emb_t *__restrict__ const const cache_t *__restrict__ const const int32_t *__restrict__ const weights_placements</div><div class="ttdef"><b>Definition</b> gen_embedding_forward_split_unweighted_codegen_cuda.cu:59</div></div>
<div class="ttc" id="agen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu_html_a4a63994c436795f993c09c5626acfb05"><div class="ttname"><a href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a></div><div class="ttdeci">template __global__ kWarpSize</div><div class="ttdef"><b>Definition</b> gen_embedding_optimizer_rowwise_adagrad_split_kernel.cu:1952</div></div>
<div class="ttc" id="anamespacenbit_html_a620ba1c7dba3e279e09759758b7a86db"><div class="ttname"><a href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a></div><div class="ttdeci">C10_HOST_DEVICE C10_ALWAYS_INLINE uint32_t div_round_up(uint32_t a, uint32_t b)</div><div class="ttdef"><b>Definition</b> embedding_common.h:94</div></div>
</div><!-- fragment -->
</div>
</div>
<a id="ae6cc33dae61d3333c3d2e6be5f9cf16e" name="ae6cc33dae61d3333c3d2e6be5f9cf16e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6cc33dae61d3333c3d2e6be5f9cf16e">&#9670;&#160;</a></span>X <span class="overload">[2/6]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define X</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">DeviceOnly, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">OutputRowsPerThread, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">InputRowsInFlight, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MinNum128BRows, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MaxNum128BRows</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">    nbit::INT4_split_embedding{{ <span class="stringliteral">&quot;_nobag&quot;</span> <span class="keywordflow">if</span> nobag <span class="keywordflow">else</span> <span class="stringliteral">&quot;&quot;</span> }}_codegen_forward_{{ wdesc }}_kernel_small_L&lt;index_t, output_t, OutputRowsPerThread, kWarpsPerBlock, InputRowsInFlight, MinNum128BRows, MaxNum128BRows, DeviceOnly&gt;&lt;&lt;&lt; \</div>
<div class="line">        nbit::div_round_up(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a> * <a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread), kWarpsPerBlock), \</div>
<div class="line">        dim3(<a class="code hl_variable" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a>, kWarpsPerBlock), \</div>
<div class="line">        0, \</div>
<div class="line">        at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;( \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a>, int32_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, weights_tys, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, int32_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">else</span> %} \</div>
<div class="line">        <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a>, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        FixedDivisor(<a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread)), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, index_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>, index_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        pooling_mode, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        row_alignment, \</div>
<div class="line">        {% <span class="keywordflow">if</span> weighted %} <a class="code hl_define" href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a>(func_name2, indice_weights, <span class="keywordtype">float</span>, 1, 32), {% endif %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a>, output_t, 2, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 2, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name2, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, int32_t, 1, 32) \</div>
<div class="line">    ); \</div>
<div class="line">    C10_CUDA_KERNEL_LAUNCH_CHECK(); \</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae6cc33dae61d3333c3d2e6be5f9cf16e" name="ae6cc33dae61d3333c3d2e6be5f9cf16e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6cc33dae61d3333c3d2e6be5f9cf16e">&#9670;&#160;</a></span>X <span class="overload">[3/6]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define X</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">DeviceOnly, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">OutputRowsPerThread, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">InputRowsInFlight, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MinNum128BRows, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MaxNum128BRows</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">    nbit::INT8_split_embedding{{ <span class="stringliteral">&quot;_nobag&quot;</span> <span class="keywordflow">if</span> nobag <span class="keywordflow">else</span> <span class="stringliteral">&quot;&quot;</span> }}_codegen_forward_{{ wdesc }}_kernel_small_L&lt;index_t, output_t, OutputRowsPerThread, kWarpsPerBlock, InputRowsInFlight, MinNum128BRows, MaxNum128BRows, DeviceOnly&gt;&lt;&lt;&lt; \</div>
<div class="line">        nbit::div_round_up(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a> * <a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread), kWarpsPerBlock), \</div>
<div class="line">        dim3(<a class="code hl_variable" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a>, kWarpsPerBlock), \</div>
<div class="line">        0, \</div>
<div class="line">        at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;( \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a>, int32_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, weights_tys, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, int32_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">else</span> %} \</div>
<div class="line">        <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a>, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        FixedDivisor(<a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread)), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, index_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>, index_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        pooling_mode, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        row_alignment, \</div>
<div class="line">        {% <span class="keywordflow">if</span> weighted %} <a class="code hl_define" href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a>(func_name3, indice_weights, <span class="keywordtype">float</span>, 1, 32), {% endif %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a>, output_t, 2, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 2, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name3, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, int32_t, 1, 32) \</div>
<div class="line">    ); \</div>
<div class="line">    C10_CUDA_KERNEL_LAUNCH_CHECK(); \</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae6cc33dae61d3333c3d2e6be5f9cf16e" name="ae6cc33dae61d3333c3d2e6be5f9cf16e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6cc33dae61d3333c3d2e6be5f9cf16e">&#9670;&#160;</a></span>X <span class="overload">[4/6]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define X</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">DeviceOnly, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">OutputRowsPerThread, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">InputRowsInFlight, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MinNum128BRows, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MaxNum128BRows</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">    nbit::FP8_split_embedding{{ <span class="stringliteral">&quot;_nobag&quot;</span> <span class="keywordflow">if</span> nobag <span class="keywordflow">else</span> <span class="stringliteral">&quot;&quot;</span> }}_codegen_forward_{{ wdesc }}_kernel_small_L&lt;index_t, output_t, OutputRowsPerThread, kWarpsPerBlock, InputRowsInFlight, MinNum128BRows, MaxNum128BRows, DeviceOnly&gt;&lt;&lt;&lt; \</div>
<div class="line">        nbit::div_round_up(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a> * <a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread), kWarpsPerBlock), \</div>
<div class="line">        dim3(<a class="code hl_variable" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a>, kWarpsPerBlock), \</div>
<div class="line">        0, \</div>
<div class="line">        at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;( \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a>, int32_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, weights_tys, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, int32_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">else</span> %} \</div>
<div class="line">        <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a>, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        FixedDivisor(<a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread)), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, index_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>, index_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        pooling_mode, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        row_alignment, \</div>
<div class="line">        {% <span class="keywordflow">if</span> weighted %} <a class="code hl_define" href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a>(func_name4, indice_weights, <span class="keywordtype">float</span>, 1, 32), {% endif %} \</div>
<div class="line">        fp8_exponent_bits, \</div>
<div class="line">        fp8_exponent_bias, \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a>, output_t, 2, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 2, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name4, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, int32_t, 1, 32) \</div>
<div class="line">    ); \</div>
<div class="line">    C10_CUDA_KERNEL_LAUNCH_CHECK(); \</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae6cc33dae61d3333c3d2e6be5f9cf16e" name="ae6cc33dae61d3333c3d2e6be5f9cf16e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6cc33dae61d3333c3d2e6be5f9cf16e">&#9670;&#160;</a></span>X <span class="overload">[5/6]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define X</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">DeviceOnly, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">OutputRowsPerThread, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">InputRowsInFlight, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MinNum128BRows, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MaxNum128BRows</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">    nbit::FP16_split_embedding{{ <span class="stringliteral">&quot;_nobag&quot;</span> <span class="keywordflow">if</span> nobag <span class="keywordflow">else</span> <span class="stringliteral">&quot;&quot;</span> }}_codegen_forward_{{ wdesc }}_kernel_small_L&lt;index_t, output_t, OutputRowsPerThread, kWarpsPerBlock, InputRowsInFlight, MinNum128BRows, MaxNum128BRows, DeviceOnly&gt;&lt;&lt;&lt; \</div>
<div class="line">        nbit::div_round_up(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a> * <a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread), kWarpsPerBlock), \</div>
<div class="line">        dim3(<a class="code hl_variable" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a>, kWarpsPerBlock), \</div>
<div class="line">        0, \</div>
<div class="line">        at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;( \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a>, int32_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, weights_tys, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, int32_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">else</span> %} \</div>
<div class="line">        <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a>, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        FixedDivisor(<a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread)), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, index_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>, index_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        pooling_mode, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        row_alignment, \</div>
<div class="line">        {% <span class="keywordflow">if</span> weighted %} <a class="code hl_define" href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a>(func_name5, indice_weights, <span class="keywordtype">float</span>, 1, 32), {% endif %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a>, output_t, 2, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 2, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name5, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, int32_t, 1, 32) \</div>
<div class="line">    ); \</div>
<div class="line">    C10_CUDA_KERNEL_LAUNCH_CHECK(); \</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae6cc33dae61d3333c3d2e6be5f9cf16e" name="ae6cc33dae61d3333c3d2e6be5f9cf16e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6cc33dae61d3333c3d2e6be5f9cf16e">&#9670;&#160;</a></span>X <span class="overload">[6/6]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define X</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">DeviceOnly, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">OutputRowsPerThread, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">InputRowsInFlight, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MinNum128BRows, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname">MaxNum128BRows</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">    nbit::FP32_split_embedding{{ <span class="stringliteral">&quot;_nobag&quot;</span> <span class="keywordflow">if</span> nobag <span class="keywordflow">else</span> <span class="stringliteral">&quot;&quot;</span> }}_codegen_forward_{{ wdesc }}_kernel_small_L&lt;index_t, output_t, OutputRowsPerThread, kWarpsPerBlock, InputRowsInFlight, MinNum128BRows, MaxNum128BRows, DeviceOnly&gt;&lt;&lt;&lt; \</div>
<div class="line">        nbit::div_round_up(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a2ee4b3e799d56c4d34c87190c37a7a64">T</a> * <a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">nbit::div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread), kWarpsPerBlock), \</div>
<div class="line">        dim3(<a class="code hl_variable" href="gen__embedding__optimizer__rowwise__adagrad__split__kernel_8cu.html#a4a63994c436795f993c09c5626acfb05">kWarpSize</a>, kWarpsPerBlock), \</div>
<div class="line">        0, \</div>
<div class="line">        at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;( \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a6d8072fe7f1cbd1cf456e3ea8a440ad3">dev_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a17f61eb7bf7a7e4089982fbf69116da5">uvm_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#ad4dd9cc51f1eccdf4626318632701868">weights_placements</a>, int32_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a764f8ae801cd000c2a5cb4bb23f14299">weights_offsets</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#ac4ebc0de2e60165af8333b6f4eab3e70">int64_t</a>, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, weights_tys, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a8a3ac708f5fc38ea5ebecdbe685f3c73">D_offsets</a>, int32_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">else</span> %} \</div>
<div class="line">        <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#a057f5488fcdaf454d09c4f1b25374ac9">D</a>, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        FixedDivisor(<a class="code hl_function" href="namespacenbit.html#a620ba1c7dba3e279e09759758b7a86db">div_round_up</a>(<a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a240b4e029c521f922d447346c8b757b8">B</a>, OutputRowsPerThread)), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#acbf20500022fb5f972956bea423a05ff">indices</a>, index_t, 1, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__embedding__forward__dense__unweighted__nobag__kernel__small_8cu.html#aff2584a62b3409906c19c5419a4cc647">offsets</a>, index_t, 1, 32), \</div>
<div class="line">        {% <span class="keywordflow">if</span> not nobag %} \</div>
<div class="line">        pooling_mode, \</div>
<div class="line">        {% endif %} \</div>
<div class="line">        row_alignment, \</div>
<div class="line">        {% <span class="keywordflow">if</span> weighted %} <a class="code hl_define" href="fbgemm__tensor__accessor_8h.html#a614f4b016e2758186bd598bc3be6e6cf">MAKE_PTA_WITH_NAME</a>(func_name6, indice_weights, <span class="keywordtype">float</span>, 1, 32), {% endif %} \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel__small_8cu.html#a110a71f81fecd3888738618492db1672">output</a>, output_t, 2, 32), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a0c2527424502280dfcf6276b49b41cdc">lxu_cache_weights</a>, <a class="code hl_variable" href="gen__batch__index__select__dim0__forward__kernel_8cu.html#a1360e7840ee58417b26bf9445f94c59d">uint8_t</a>, 2, 64), \</div>
<div class="line">        MAKE_PTA_WITH_NAME(func_name6, <a class="code hl_variable" href="gen__embedding__forward__split__unweighted__codegen__cuda_8cu.html#a60a1ec59d36df78e844d5cd7a0d34f03">lxu_cache_locations</a>, int32_t, 1, 32) \</div>
<div class="line">    ); \</div>
<div class="line">    C10_CUDA_KERNEL_LAUNCH_CHECK(); \</div>
</div><!-- fragment -->
</div>
</div>
<a id="acec51faeb0681c58de451cb9d59abe95" name="acec51faeb0681c58de451cb9d59abe95"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acec51faeb0681c58de451cb9d59abe95">&#9670;&#160;</a></span>Y</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define Y</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>...</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">      <span class="keywordflow">if</span> (device_only) { \</div>
<div class="line">        X(<span class="keyword">true</span>, __VA_ARGS__) \</div>
<div class="line">      } <span class="keywordflow">else</span> { \</div>
<div class="line">        X(<span class="keyword">false</span>, __VA_ARGS__) \</div>
<div class="line">      };</div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="a1c03911dcc4fa0b0d2819531e1148a4f" name="a1c03911dcc4fa0b0d2819531e1148a4f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1c03911dcc4fa0b0d2819531e1148a4f">&#9670;&#160;</a></span>Tensor</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="gen__batch__index__select__dim0__backward__codegen__cuda_8cu.html#abc1167888f441327c12e300780ee568a">Tensor</a></td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line">{% set wdesc =  <span class="stringliteral">&quot;weighted&quot;</span> <span class="keywordflow">if</span> weighted <span class="keywordflow">else</span> <span class="stringliteral">&quot;unweighted&quot;</span> %}</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">using namespace </span><a class="code hl_namespace" href="namespacefbgemm__gpu.html">fbgemm_gpu</a> at::Tensor</div>
<div class="ttc" id="anamespacefbgemm__gpu_html"><div class="ttname"><a href="namespacefbgemm__gpu.html">fbgemm_gpu</a></div><div class="ttdef"><b>Definition</b> embedding_ops_placeholder.cpp:15</div></div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0
</small></address>
</body>
</html>
